{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30f90a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 2.2.0.dev20231027+cu121\n",
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "print('pytorch version:', torch.__version__)\n",
    "global_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', global_device)\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torch.nn.utils.rnn import pack_sequence, pad_packed_sequence, pad_sequence, pack_padded_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchmetrics.classification import MultilabelAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91f28206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib settings\n",
    "import matplotlib\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib.colors import colorConverter\n",
    "\n",
    "titlesize = 20\n",
    "labelsize = 16\n",
    "legendsize = labelsize\n",
    "xticksize = 14\n",
    "yticksize = xticksize\n",
    "\n",
    "matplotlib.rcParams['legend.markerscale'] = 1.5     # the relative size of legend markers vs. original\n",
    "matplotlib.rcParams['legend.handletextpad'] = 0.5\n",
    "matplotlib.rcParams['legend.labelspacing'] = 0.4    # the vertical space between the legend entries in fraction of fontsize\n",
    "matplotlib.rcParams['legend.borderpad'] = 0.5       # border whitespace in fontsize units\n",
    "matplotlib.rcParams['font.size'] = 12\n",
    "matplotlib.rcParams['font.family'] = 'serif'\n",
    "matplotlib.rcParams['font.serif'] = 'Times New Roman'\n",
    "matplotlib.rcParams['axes.labelsize'] = labelsize\n",
    "matplotlib.rcParams['axes.titlesize'] = titlesize\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=xticksize)\n",
    "matplotlib.rc('ytick', labelsize=yticksize)\n",
    "matplotlib.rc('legend', fontsize=legendsize)\n",
    "\n",
    "matplotlib.rc('font', **{'family':'serif'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94d3879",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "487259db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trainval = pd.read_csv('data/train.csv')\n",
    "df_train, df_val = train_test_split(df_trainval, test_size=0.2, random_state=42)\n",
    "df_trainval['tags'] = df_trainval['tags'].apply(lambda x: np.array(list(map(int, x.split(',')))))\n",
    "df_train['tags'] = df_train['tags'].apply(lambda x: np.array(list(map(int, x.split(',')))))\n",
    "df_val['tags'] = df_val['tags'].apply(lambda x: np.array(list(map(int, x.split(',')))))\n",
    "\n",
    "df_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "207ca2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76714/76714 [01:06<00:00, 1147.95it/s]\n"
     ]
    }
   ],
   "source": [
    "global_idx2embeds = {} # {idx: np.ndarray[n, 768])}\n",
    "for npy_file in tqdm(glob('data/track_embeddings/*')):\n",
    "    track_idx = int(npy_file.split('\\\\')[1].split('.')[0])\n",
    "    embeds = np.load(npy_file)\n",
    "    global_idx2embeds[track_idx] = embeds[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cbe31ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51134/51134 [00:28<00:00, 1809.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_train_mean = 0\n",
    "for track_idx in tqdm(df_trainval['track']):\n",
    "    emb_train_mean += global_idx2embeds[track_idx].mean(axis=0)\n",
    "emb_train_mean /= len(df_trainval['track'])\n",
    "emb_train_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d838b48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_tags, test=False):\n",
    "        self.df_tags = df_tags\n",
    "        self.test = test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_tags)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        track_idx = self.df_tags.iloc[idx]['track']\n",
    "        embeds = global_idx2embeds[track_idx]\n",
    "        if self.test:\n",
    "            return track_idx, embeds\n",
    "        labels_onehot = np.zeros(256)\n",
    "        labels_onehot[self.df_tags.iloc[idx]['tags']] = 1\n",
    "        return track_idx, embeds, labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85173d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    track_idxs = torch.IntTensor(np.vstack([triplet[0] for triplet in batch]))\n",
    "    embeds_list = [torch.FloatTensor(triplet[1][:-2] - emb_train_mean).to(global_device) for triplet in batch]\n",
    "    embeds = pack_sequence(embeds_list, enforce_sorted=False).to(global_device)\n",
    "    labels_onehot = torch.FloatTensor(np.vstack([triplet[2] for triplet in batch])).to(global_device)\n",
    "    return track_idxs, embeds, labels_onehot\n",
    "\n",
    "def collate_test(batch):\n",
    "    track_idxs = torch.IntTensor(np.vstack([triplet[0] for triplet in batch]))\n",
    "    embeds_list = [torch.FloatTensor(triplet[1][:-2] - emb_train_mean) for triplet in batch]\n",
    "    embeds = pack_sequence(embeds_list, enforce_sorted=False).to(global_device)\n",
    "    return track_idxs, embeds\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "dataset_train = TrackDataset(df_train)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size, shuffle=True, collate_fn=collate)\n",
    "\n",
    "dataset_trainval = TrackDataset(df_trainval)\n",
    "dataloader_trainval = DataLoader(dataset_trainval, batch_size, shuffle=True, collate_fn=collate)\n",
    "\n",
    "dataset_val = TrackDataset(df_val)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size, shuffle=False, collate_fn=collate)\n",
    "dataloader_val_astest = DataLoader(dataset_val, batch_size, shuffle=False, collate_fn=collate_test)\n",
    "\n",
    "dataset_test = TrackDataset(df_test, test=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size, shuffle=False, collate_fn=collate_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68675c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackToTags(pl.LightningModule):\n",
    "    def __init__(self, num_classes=256, input_dim=768, hidden_dim=512, relu_on=True,\n",
    "                 grulayers=2, gru_in_out=[(768, 768), (768, 768)], poolkernels=[(3,1), (3,1), (3,1)]):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.grulayers = grulayers\n",
    "        self.poolkernels = poolkernels\n",
    "        self.gru_in_out = gru_in_out\n",
    "        self.relu_on = relu_on\n",
    "        gru_list = []\n",
    "        for i in range(self.grulayers):\n",
    "            gru_list.append(nn.GRU(gru_in_out[i][0], gru_in_out[i][1], batch_first=True, bidirectional=False, \n",
    "                                   num_layers=1).to(global_device))\n",
    "        \n",
    "        self.grus = torch.nn.ModuleList(modules=gru_list)\n",
    "        \n",
    "        pool_list = []\n",
    "        for kernel in poolkernels:\n",
    "            pool_list.append(torch.nn.MaxPool2d(kernel_size=kernel, \n",
    "                                                padding=(kernel[0]//2,kernel[1]//2)).to(global_device))\n",
    "        self.maxpools = torch.nn.ModuleList(modules=pool_list)\n",
    "\n",
    "        self.bn = nn.LayerNorm(gru_in_out[-1][1])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(gru_in_out[-1][1], num_classes)\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        self.metric = average_precision_score\n",
    "        self._transit_val = {'preds': [], 'labels': []}\n",
    "        \n",
    "    def forward(self, embeds):\n",
    "        unpacked, lengths = pad_packed_sequence(embeds, batch_first=True)\n",
    "        x = unpacked\n",
    "        x = self.maxpools[0](unpacked)\n",
    "        for layer in range(self.grulayers):\n",
    "            new_lengths = np.ceil(lengths/self.poolkernels[layer][0])\n",
    "            x = pack_padded_sequence(x, new_lengths, \n",
    "                                     batch_first=True, enforce_sorted=False)\n",
    "            gruout, h = self.grus[layer](x)\n",
    "            unpacked, lengths = pad_packed_sequence(gruout, batch_first=True)\n",
    "            x = self.maxpools[layer+1](unpacked)\n",
    "                \n",
    "        x = [(v.sum(0)/length*self.poolkernels[-1][0]).unsqueeze(0) for v, length in zip(x, lengths)]\n",
    "        x = torch.cat(x, dim = 0)\n",
    "        x = self.bn(x)\n",
    "        if self.relu_on:\n",
    "            x = self.relu(x)\n",
    "        outs = self.fc(x)\n",
    "        return outs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        track_idxs, embeds_list, labels_onehot = batch\n",
    "        pred_logits = self(embeds_list)\n",
    "        loss = self.loss(pred_logits, labels_onehot)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        track_idxs, embeds_list, labels_onehot = batch\n",
    "        pred_logits = self(embeds_list)\n",
    "        loss = self.loss(pred_logits, labels_onehot)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        \n",
    "        pred_probs = torch.sigmoid(pred_logits)\n",
    "        self._transit_val['labels'].append(np.array(labels_onehot.int().cpu()))\n",
    "        self._transit_val['preds'].append(np.array(pred_probs.cpu()))\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        preds = np.vstack(self._transit_val['preds'])\n",
    "        labels = np.vstack(self._transit_val['labels'])\n",
    "        ap = self.metric(labels, preds)\n",
    "        print(ap)\n",
    "        self.log('val_ap', ap, prog_bar=True)\n",
    "        self._transit_val['labels'] = []\n",
    "        self._transit_val['preds'] = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=4e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)\n",
    "        \n",
    "        return {\"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                                \"scheduler\": scheduler,\n",
    "                                \"interval\": \"epoch\",\n",
    "                                \"frequency\": 10\n",
    "                                },\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e2bb621",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type              | Params\n",
      "-----------------------------------------------\n",
      "0 | grus     | ModuleList        | 2.0 M \n",
      "1 | maxpools | ModuleList        | 0     \n",
      "2 | bn       | LayerNorm         | 1.0 K \n",
      "3 | relu     | ReLU              | 0     \n",
      "4 | fc       | Linear            | 131 K \n",
      "5 | loss     | BCEWithLogitsLoss | 0     \n",
      "-----------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.406     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  4.60it/s]0.4247182317221616\n",
      "Epoch 0: 100%|██████████| 200/200 [03:08<00:00,  1.06it/s, v_num=298, train_loss=0.0389]0.3430559311932406\n",
      "Epoch 1: 100%|██████████| 200/200 [02:34<00:00,  1.29it/s, v_num=298, train_loss=0.0376, val_loss=0.0385, val_ap=0.343]0.35701855526832077\n",
      "Epoch 2: 100%|██████████| 200/200 [02:29<00:00,  1.34it/s, v_num=298, train_loss=0.0363, val_loss=0.0376, val_ap=0.357]0.3683581085262343\n",
      "Epoch 3: 100%|██████████| 200/200 [02:14<00:00,  1.49it/s, v_num=298, train_loss=0.0403, val_loss=0.037, val_ap=0.368] 0.3739574563211061\n",
      "Epoch 4: 100%|██████████| 200/200 [02:13<00:00,  1.49it/s, v_num=298, train_loss=0.0345, val_loss=0.0366, val_ap=0.374]0.3857148725092646\n",
      "Epoch 4: 100%|██████████| 200/200 [02:23<00:00,  1.39it/s, v_num=298, train_loss=0.0345, val_loss=0.0361, val_ap=0.386]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 200/200 [02:23<00:00,  1.39it/s, v_num=298, train_loss=0.0345, val_loss=0.0361, val_ap=0.386]\n"
     ]
    }
   ],
   "source": [
    "poolkernels = [(1, 1), (1, 1)]  # (1,1) = no pooling\n",
    "gru_in_out = [(768, 512)]\n",
    "layers = (len(gru_in_out))\n",
    "io_log = '-'.join([f\"{io[0]}x{io[1]}\" for io in gru_in_out])\n",
    "kernelslog = '-'.join([f\"{kernel[0]}x{kernel[1]}\" for kernel in poolkernels])\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=f'lightning_logs/trv2_grupool_l-{layers}_gruio-{io_log}_krnls-{kernelslog}/',\n",
    "                                      filename='{epoch}-{val_loss:.3f}-{val_ap:.3f}', \n",
    "                                      save_top_k=-1, \n",
    "                                      monitor=\"val_ap\", \n",
    "                                      every_n_epochs=1)\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=1, val_check_interval=1.0, \n",
    "                     max_epochs=5, log_every_n_steps=100,\n",
    "                     callbacks=[checkpoint_callback])\n",
    "# model_1 = TrackToTags(grulayers=layers, gru_in_out=gru_in_out, poolkernels=poolkernels)\n",
    "trainer.fit(model_1, dataloader_trainval, dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(pl.LightningModule):\n",
    "    def __init__(self, num_classes=256, input_dim=768, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.bn = nn.LayerNorm(hidden_dim)\n",
    "        self.projector =  nn.Linear(input_dim, hidden_dim)\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "        self.pos_weights = torch.FloatTensor(np.ones(256)).to(global_device)\n",
    "        self.loss = nn.BCEWithLogitsLoss(pos_weight=self.pos_weights)\n",
    "        self.metric = average_precision_score\n",
    "        self._transit_val = {'preds': [], 'labels': []}\n",
    "        \n",
    "    def forward(self, embeds_list):\n",
    "        unpacked, lengths = pad_packed_sequence(embeds_list, batch_first=True)\n",
    "        x = [self.projector(x) for x in unpacked]\n",
    "        x = [(v.sum(0)/length).unsqueeze(0) for v, length in zip(x, lengths)]\n",
    "        x = self.bn(torch.cat(x, dim = 0))\n",
    "        x = self.lin(x)\n",
    "        outs = self.fc(x)\n",
    "        return outs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        track_idxs, embeds_list, labels_onehot = batch\n",
    "        pred_logits = self(embeds_list)\n",
    "        loss = self.loss(pred_logits, labels_onehot)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        track_idxs, embeds_list, labels_onehot = batch\n",
    "        pred_logits = self(embeds_list)\n",
    "        loss = self.loss(pred_logits, labels_onehot)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        \n",
    "        pred_probs = torch.sigmoid(pred_logits)\n",
    "        self._transit_val['labels'].append(np.array(labels_onehot.int().cpu()))\n",
    "        self._transit_val['preds'].append(np.array(pred_probs.cpu()))\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        preds = np.vstack(self._transit_val['preds'])\n",
    "        labels = np.vstack(self._transit_val['labels'])\n",
    "        ap = self.metric(labels, preds)\n",
    "        self.log('val_ap', ap, prog_bar=True)\n",
    "        self._transit_val['labels'] = []\n",
    "        self._transit_val['preds'] = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=3e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)\n",
    "        \n",
    "        return {\"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                                \"scheduler\": scheduler,\n",
    "                                \"interval\": \"epoch\",\n",
    "                                \"frequency\": 10\n",
    "                                },\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | bn        | LayerNorm         | 1.0 K \n",
      "1 | projector | Linear            | 393 K \n",
      "2 | lin       | Sequential        | 526 K \n",
      "3 | fc        | Linear            | 131 K \n",
      "4 | loss      | BCEWithLogitsLoss | 0     \n",
      "------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.210     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 160/160 [01:13<00:00,  2.19it/s, v_num=222, train_loss=0.0421, val_loss=0.047, val_ap=0.216] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 160/160 [01:13<00:00,  2.19it/s, v_num=222, train_loss=0.0421, val_loss=0.047, val_ap=0.216]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(dirpath=f'lightning_logs/baseline/',\n",
    "                                      filename='{epoch}-{val_loss:.3f}-{val_ap:.3f}', \n",
    "                                      save_top_k=-1, \n",
    "                                      monitor=\"val_ap\", \n",
    "                                      every_n_epochs=1)\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=1, val_check_interval=1.0, \n",
    "                     max_epochs=10, log_every_n_steps=100,\n",
    "                     callbacks=[checkpoint_callback])\n",
    "# model_2 = Baseline()\n",
    "trainer.fit(model_2, dataloader_train, dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type              | Params\n",
      "-----------------------------------------------\n",
      "0 | grus     | ModuleList        | 3.5 M \n",
      "1 | maxpools | ModuleList        | 0     \n",
      "2 | bn       | LayerNorm         | 1.0 K \n",
      "3 | relu     | ReLU              | 0     \n",
      "4 | fc       | Linear            | 131 K \n",
      "5 | loss     | BCEWithLogitsLoss | 0     \n",
      "-----------------------------------------------\n",
      "3.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.7 M     Total params\n",
      "14.710    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:01<00:00,  1.72it/s]0.023533502157419863\n",
      "Epoch 0: 100%|██████████| 200/200 [03:16<00:00,  1.02it/s, v_num=297, train_loss=0.061] 0.04600947330770277\n",
      "Epoch 1: 100%|██████████| 200/200 [02:53<00:00,  1.15it/s, v_num=297, train_loss=0.0538, val_loss=0.061, val_ap=0.046]0.0919068976875282\n",
      "Epoch 2: 100%|██████████| 200/200 [02:50<00:00,  1.17it/s, v_num=297, train_loss=0.0526, val_loss=0.0546, val_ap=0.0919]0.12209936560673228\n",
      "Epoch 3: 100%|██████████| 200/200 [02:50<00:00,  1.17it/s, v_num=297, train_loss=0.0464, val_loss=0.0514, val_ap=0.122] 0.15568521138382102\n",
      "Epoch 4: 100%|██████████| 200/200 [02:47<00:00,  1.20it/s, v_num=297, train_loss=0.0472, val_loss=0.0486, val_ap=0.156]0.17898199301364406\n",
      "Epoch 5: 100%|██████████| 200/200 [02:36<00:00,  1.27it/s, v_num=297, train_loss=0.0448, val_loss=0.047, val_ap=0.179] 0.20233085723841893\n",
      "Epoch 6: 100%|██████████| 200/200 [02:35<00:00,  1.28it/s, v_num=297, train_loss=0.0462, val_loss=0.0456, val_ap=0.202]0.22165001230606987\n",
      "Epoch 7: 100%|██████████| 200/200 [02:46<00:00,  1.20it/s, v_num=297, train_loss=0.043, val_loss=0.0446, val_ap=0.222] 0.23310356374634184\n",
      "Epoch 8: 100%|██████████| 200/200 [02:49<00:00,  1.18it/s, v_num=297, train_loss=0.0448, val_loss=0.0438, val_ap=0.233]0.25261095945315687\n",
      "Epoch 9: 100%|██████████| 200/200 [02:49<00:00,  1.18it/s, v_num=297, train_loss=0.0395, val_loss=0.0431, val_ap=0.253]0.26794674675289487\n",
      "Epoch 10: 100%|██████████| 200/200 [02:45<00:00,  1.21it/s, v_num=297, train_loss=0.0382, val_loss=0.0421, val_ap=0.268]0.28808111424241134\n",
      "Epoch 11: 100%|██████████| 200/200 [02:35<00:00,  1.28it/s, v_num=297, train_loss=0.0393, val_loss=0.0408, val_ap=0.288]0.29560521559031727\n",
      "Epoch 12: 100%|██████████| 200/200 [02:46<00:00,  1.20it/s, v_num=297, train_loss=0.0406, val_loss=0.0404, val_ap=0.296]0.3078314705107856\n",
      "Epoch 13: 100%|██████████| 200/200 [02:49<00:00,  1.18it/s, v_num=297, train_loss=0.0417, val_loss=0.0399, val_ap=0.308]0.31495659045286273\n",
      "Epoch 14: 100%|██████████| 200/200 [02:49<00:00,  1.18it/s, v_num=297, train_loss=0.0397, val_loss=0.0395, val_ap=0.315]0.3265566637047127\n",
      "Epoch 15: 100%|██████████| 200/200 [02:45<00:00,  1.21it/s, v_num=297, train_loss=0.0399, val_loss=0.0389, val_ap=0.327]0.3341371980046394\n",
      "Epoch 16: 100%|██████████| 200/200 [02:40<00:00,  1.24it/s, v_num=297, train_loss=0.0404, val_loss=0.0387, val_ap=0.334]0.3493038118171442\n",
      "Epoch 17: 100%|██████████| 200/200 [02:49<00:00,  1.18it/s, v_num=297, train_loss=0.0391, val_loss=0.0378, val_ap=0.349]0.3598639629396354\n",
      "Epoch 18: 100%|██████████| 200/200 [02:49<00:00,  1.18it/s, v_num=297, train_loss=0.0379, val_loss=0.0373, val_ap=0.360]0.3676708671385496\n",
      "Epoch 19: 100%|██████████| 200/200 [02:48<00:00,  1.18it/s, v_num=297, train_loss=0.0363, val_loss=0.0368, val_ap=0.368]0.3801227485212213\n",
      "Epoch 20: 100%|██████████| 200/200 [02:50<00:00,  1.18it/s, v_num=297, train_loss=0.0364, val_loss=0.0362, val_ap=0.380]0.3984553489335701\n",
      "Epoch 21: 100%|██████████| 200/200 [02:52<00:00,  1.16it/s, v_num=297, train_loss=0.0355, val_loss=0.0352, val_ap=0.398]0.40575475751849566\n",
      "Epoch 22: 100%|██████████| 200/200 [02:50<00:00,  1.18it/s, v_num=297, train_loss=0.0342, val_loss=0.0349, val_ap=0.406]0.4138833262900866\n",
      "Epoch 22: 100%|██████████| 200/200 [03:03<00:00,  1.09it/s, v_num=297, train_loss=0.0342, val_loss=0.0345, val_ap=0.414]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=23` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 200/200 [03:03<00:00,  1.09it/s, v_num=297, train_loss=0.0342, val_loss=0.0345, val_ap=0.414]\n"
     ]
    }
   ],
   "source": [
    "poolkernels = [(1, 1), (3, 1), (3, 1)]  # (1,1) = no pooling\n",
    "gru_in_out = [(768, 512), (512, 512)]\n",
    "layers = (len(gru_in_out))\n",
    "io_log = '-'.join([f\"{io[0]}x{io[1]}\" for io in gru_in_out])\n",
    "kernelslog = '-'.join([f\"{kernel[0]}x{kernel[1]}\" for kernel in poolkernels])\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=f'lightning_logs/trv_grupool_l-{layers}_gruio-{io_log}_krnls-{kernelslog}/',\n",
    "                                      filename='{epoch}-{val_loss:.3f}-{val_ap:.3f}', \n",
    "                                      save_top_k=-1, \n",
    "                                      monitor=\"val_ap\", \n",
    "                                      every_n_epochs=1)\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=1, val_check_interval=1.0, \n",
    "                     max_epochs=23, log_every_n_steps=100,\n",
    "                     callbacks=[checkpoint_callback])\n",
    "model_3 = TrackToTags(grulayers=layers, gru_in_out=gru_in_out, poolkernels=poolkernels)\n",
    "trainer.fit(model_3, dataloader_trainval, dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type              | Params\n",
      "-----------------------------------------------\n",
      "0 | grus     | ModuleList        | 7.9 M \n",
      "1 | maxpools | ModuleList        | 0     \n",
      "2 | bn       | LayerNorm         | 1.0 K \n",
      "3 | relu     | ReLU              | 0     \n",
      "4 | fc       | Linear            | 131 K \n",
      "5 | loss     | BCEWithLogitsLoss | 0     \n",
      "-----------------------------------------------\n",
      "8.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.0 M     Total params\n",
      "32.024    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  3.03it/s]0.026834616652135067\n",
      "Epoch 0: 100%|██████████| 160/160 [06:25<00:00,  0.42it/s, v_num=227, train_loss=0.0635]0.042012589846153525\n",
      "Epoch 1: 100%|██████████| 160/160 [06:19<00:00,  0.42it/s, v_num=227, train_loss=0.0535, val_loss=0.0627, val_ap=0.042]0.07311260426593134\n",
      "Epoch 2: 100%|██████████| 160/160 [05:48<00:00,  0.46it/s, v_num=227, train_loss=0.0499, val_loss=0.0572, val_ap=0.0731]0.104399805347369\n",
      "Epoch 3: 100%|██████████| 160/160 [05:50<00:00,  0.46it/s, v_num=227, train_loss=0.0508, val_loss=0.0536, val_ap=0.104] 0.1263182066045544\n",
      "Epoch 4: 100%|██████████| 160/160 [05:49<00:00,  0.46it/s, v_num=227, train_loss=0.0498, val_loss=0.0515, val_ap=0.126]0.1431803416456834\n",
      "Epoch 5: 100%|██████████| 160/160 [05:54<00:00,  0.45it/s, v_num=227, train_loss=0.0497, val_loss=0.0499, val_ap=0.143]0.15909084414674485\n",
      "Epoch 6: 100%|██████████| 160/160 [07:16<00:00,  0.37it/s, v_num=227, train_loss=0.0461, val_loss=0.0484, val_ap=0.159]0.1754556956888224\n",
      "Epoch 7: 100%|██████████| 160/160 [05:12<00:00,  0.51it/s, v_num=227, train_loss=0.0475, val_loss=0.0474, val_ap=0.175]0.1876072786692785\n",
      "Epoch 8: 100%|██████████| 160/160 [05:20<00:00,  0.50it/s, v_num=227, train_loss=0.043, val_loss=0.0467, val_ap=0.188] 0.19681940585935015\n",
      "Epoch 9: 100%|██████████| 160/160 [05:15<00:00,  0.51it/s, v_num=227, train_loss=0.0452, val_loss=0.0461, val_ap=0.197]0.20366536800867654\n",
      "Epoch 10: 100%|██████████| 160/160 [05:20<00:00,  0.50it/s, v_num=227, train_loss=0.0404, val_loss=0.0457, val_ap=0.204]0.21392281114506906\n",
      "Epoch 11: 100%|██████████| 160/160 [05:23<00:00,  0.49it/s, v_num=227, train_loss=0.0397, val_loss=0.0452, val_ap=0.214]0.21922322222940957\n",
      "Epoch 12: 100%|██████████| 160/160 [05:16<00:00,  0.51it/s, v_num=227, train_loss=0.0423, val_loss=0.045, val_ap=0.219] 0.22137015701168644\n",
      "Epoch 13: 100%|██████████| 160/160 [05:10<00:00,  0.51it/s, v_num=227, train_loss=0.0402, val_loss=0.0449, val_ap=0.221]0.22570698640582137\n",
      "Epoch 14: 100%|██████████| 160/160 [05:13<00:00,  0.51it/s, v_num=227, train_loss=0.0413, val_loss=0.0446, val_ap=0.226]0.22752133407773956\n",
      "Epoch 15: 100%|██████████| 160/160 [05:18<00:00,  0.50it/s, v_num=227, train_loss=0.0407, val_loss=0.0447, val_ap=0.228]0.22830649854105484\n",
      "Epoch 16: 100%|██████████| 160/160 [05:17<00:00,  0.50it/s, v_num=227, train_loss=0.0404, val_loss=0.0449, val_ap=0.228]0.22948885074035325\n",
      "Epoch 17: 100%|██████████| 160/160 [05:11<00:00,  0.51it/s, v_num=227, train_loss=0.0404, val_loss=0.0449, val_ap=0.229]0.23187065987900912\n",
      "Epoch 18: 100%|██████████| 160/160 [05:19<00:00,  0.50it/s, v_num=227, train_loss=0.0364, val_loss=0.0449, val_ap=0.232]0.23227360667955385\n",
      "Epoch 19: 100%|██████████| 160/160 [05:13<00:00,  0.51it/s, v_num=227, train_loss=0.0359, val_loss=0.045, val_ap=0.232] 0.23090822820361726\n",
      "Epoch 19: 100%|██████████| 160/160 [05:29<00:00,  0.49it/s, v_num=227, train_loss=0.0359, val_loss=0.0455, val_ap=0.231]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 160/160 [05:29<00:00,  0.49it/s, v_num=227, train_loss=0.0359, val_loss=0.0455, val_ap=0.231]\n"
     ]
    }
   ],
   "source": [
    "poolkernels = [(1, 1), (2, 1), (2, 1)]  # (1,1) = no pooling\n",
    "gru_in_out = [(768, 1024), (1024, 512)]\n",
    "layers = (len(gru_in_out))\n",
    "io_log = '-'.join([f\"{io[0]}x{io[1]}\" for io in gru_in_out])\n",
    "kernelslog = '-'.join([f\"{kernel[0]}x{kernel[1]}\" for kernel in poolkernels])\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=f'lightning_logs/grupool_l-{layers}_gruio-{io_log}_krnls-{kernelslog}/',\n",
    "                                      filename='{epoch}-{val_loss:.3f}-{val_ap:.3f}', \n",
    "                                      save_top_k=-1, \n",
    "                                      monitor=\"val_ap\", \n",
    "                                      every_n_epochs=1)\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=1, val_check_interval=1.0, \n",
    "                     max_epochs=20, log_every_n_steps=100,\n",
    "                     callbacks=[checkpoint_callback])\n",
    "model_4 = TrackToTags(grulayers=layers, gru_in_out=gru_in_out, poolkernels=poolkernels)\n",
    "trainer.fit(model_4, dataloader_train, dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type              | Params\n",
      "-----------------------------------------------\n",
      "0 | grus     | ModuleList        | 3.5 M \n",
      "1 | maxpools | ModuleList        | 0     \n",
      "2 | bn       | LayerNorm         | 1.0 K \n",
      "3 | relu     | ReLU              | 0     \n",
      "4 | fc       | Linear            | 131 K \n",
      "5 | loss     | BCEWithLogitsLoss | 0     \n",
      "-----------------------------------------------\n",
      "3.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.7 M     Total params\n",
      "14.710    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  7.86it/s]0.028424297445527644\n",
      "Epoch 0: 100%|██████████| 160/160 [01:06<00:00,  2.39it/s, v_num=228, train_loss=0.0657]0.03212504849990009\n",
      "Epoch 1: 100%|██████████| 160/160 [01:07<00:00,  2.38it/s, v_num=228, train_loss=0.0576, val_loss=0.0642, val_ap=0.0321]0.08308654677197247\n",
      "Epoch 2: 100%|██████████| 160/160 [01:06<00:00,  2.41it/s, v_num=228, train_loss=0.0518, val_loss=0.0545, val_ap=0.0831]0.12356024093547344\n",
      "Epoch 3: 100%|██████████| 160/160 [01:06<00:00,  2.40it/s, v_num=228, train_loss=0.0486, val_loss=0.0509, val_ap=0.124] 0.149293792136317\n",
      "Epoch 4: 100%|██████████| 160/160 [01:06<00:00,  2.40it/s, v_num=228, train_loss=0.0469, val_loss=0.0491, val_ap=0.149]0.16753707320788291\n",
      "Epoch 5: 100%|██████████| 160/160 [01:06<00:00,  2.42it/s, v_num=228, train_loss=0.0476, val_loss=0.048, val_ap=0.168] 0.18126676453026047\n",
      "Epoch 6: 100%|██████████| 160/160 [01:06<00:00,  2.40it/s, v_num=228, train_loss=0.0498, val_loss=0.0473, val_ap=0.181]0.1919840284826937\n",
      "Epoch 7: 100%|██████████| 160/160 [01:06<00:00,  2.39it/s, v_num=228, train_loss=0.0472, val_loss=0.0466, val_ap=0.192]0.1997538597285288\n",
      "Epoch 8: 100%|██████████| 160/160 [01:06<00:00,  2.40it/s, v_num=228, train_loss=0.0439, val_loss=0.0463, val_ap=0.200]0.20844098058660415\n",
      "Epoch 9: 100%|██████████| 160/160 [01:06<00:00,  2.40it/s, v_num=228, train_loss=0.0425, val_loss=0.0459, val_ap=0.208]0.20747373137649325\n",
      "Epoch 10: 100%|██████████| 160/160 [01:07<00:00,  2.39it/s, v_num=228, train_loss=0.0474, val_loss=0.046, val_ap=0.207]0.2224924894038818\n",
      "Epoch 11: 100%|██████████| 160/160 [01:06<00:00,  2.40it/s, v_num=228, train_loss=0.0432, val_loss=0.045, val_ap=0.222]0.22411432422078298\n",
      "Epoch 12: 100%|██████████| 160/160 [01:07<00:00,  2.37it/s, v_num=228, train_loss=0.038, val_loss=0.0449, val_ap=0.224] 0.22494406907446277\n",
      "Epoch 13: 100%|██████████| 160/160 [01:06<00:00,  2.40it/s, v_num=228, train_loss=0.0417, val_loss=0.0449, val_ap=0.225]0.22831937990463413\n",
      "Epoch 14: 100%|██████████| 160/160 [01:06<00:00,  2.40it/s, v_num=228, train_loss=0.0417, val_loss=0.0448, val_ap=0.228]0.22932842378224422\n",
      "Epoch 15: 100%|██████████| 160/160 [01:06<00:00,  2.40it/s, v_num=228, train_loss=0.0368, val_loss=0.0449, val_ap=0.229]0.23077085200455927\n",
      "Epoch 16: 100%|██████████| 160/160 [01:06<00:00,  2.39it/s, v_num=228, train_loss=0.0384, val_loss=0.0448, val_ap=0.231]0.23119924094639765\n",
      "Epoch 17: 100%|██████████| 160/160 [01:06<00:00,  2.40it/s, v_num=228, train_loss=0.0385, val_loss=0.0449, val_ap=0.231]0.23195040734415093\n",
      "Epoch 18: 100%|██████████| 160/160 [01:06<00:00,  2.40it/s, v_num=228, train_loss=0.0387, val_loss=0.0448, val_ap=0.232]0.23286766529809752\n",
      "Epoch 19: 100%|██████████| 160/160 [01:06<00:00,  2.39it/s, v_num=228, train_loss=0.0375, val_loss=0.0449, val_ap=0.233]0.2307175288974904\n",
      "Epoch 19: 100%|██████████| 160/160 [01:15<00:00,  2.12it/s, v_num=228, train_loss=0.0375, val_loss=0.0451, val_ap=0.231]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 160/160 [01:15<00:00,  2.11it/s, v_num=228, train_loss=0.0375, val_loss=0.0451, val_ap=0.231]\n"
     ]
    }
   ],
   "source": [
    "poolkernels = [(2, 1), (2, 1), (2, 1)]  # (1,1) = no pooling\n",
    "gru_in_out = [(768, 512), (512, 512)]\n",
    "layers = (len(gru_in_out))\n",
    "io_log = '-'.join([f\"{io[0]}x{io[1]}\" for io in gru_in_out])\n",
    "kernelslog = '-'.join([f\"{kernel[0]}x{kernel[1]}\" for kernel in poolkernels])\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=f'lightning_logs/grupool_l-{layers}_gruio-{io_log}_krnls-{kernelslog}/',\n",
    "                                      filename='{epoch}-{val_loss:.3f}-{val_ap:.3f}', \n",
    "                                      save_top_k=-1, \n",
    "                                      monitor=\"val_ap\", \n",
    "                                      every_n_epochs=1)\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=1, val_check_interval=1.0, \n",
    "                     max_epochs=20, log_every_n_steps=100,\n",
    "                     callbacks=[checkpoint_callback])\n",
    "model_5 = TrackToTags(grulayers=layers, gru_in_out=gru_in_out, poolkernels=poolkernels)\n",
    "trainer.fit(model_5, dataloader_train, dataloader_val)\n",
    "del model_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type              | Params\n",
      "-----------------------------------------------\n",
      "0 | grus     | ModuleList        | 5.1 M \n",
      "1 | maxpools | ModuleList        | 0     \n",
      "2 | bn       | LayerNorm         | 1.0 K \n",
      "3 | relu     | ReLU              | 0     \n",
      "4 | fc       | Linear            | 131 K \n",
      "5 | loss     | BCEWithLogitsLoss | 0     \n",
      "-----------------------------------------------\n",
      "5.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.3 M     Total params\n",
      "21.014    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  7.27it/s]0.03027342150190565\n",
      "Epoch 0: 100%|██████████| 160/160 [01:12<00:00,  2.20it/s, v_num=229, train_loss=0.0641]0.034793783371849824\n",
      "Epoch 1: 100%|██████████| 160/160 [01:13<00:00,  2.18it/s, v_num=229, train_loss=0.057, val_loss=0.064, val_ap=0.0348] 0.07484752936692989\n",
      "Epoch 2: 100%|██████████| 160/160 [01:12<00:00,  2.20it/s, v_num=229, train_loss=0.0525, val_loss=0.0572, val_ap=0.0748]0.10504454123666762\n",
      "Epoch 3: 100%|██████████| 160/160 [01:13<00:00,  2.19it/s, v_num=229, train_loss=0.0519, val_loss=0.0533, val_ap=0.105] 0.1311691810376974\n",
      "Epoch 4: 100%|██████████| 160/160 [01:12<00:00,  2.20it/s, v_num=229, train_loss=0.0493, val_loss=0.0506, val_ap=0.131]0.15470310532867043\n",
      "Epoch 5: 100%|██████████| 160/160 [01:12<00:00,  2.21it/s, v_num=229, train_loss=0.051, val_loss=0.0488, val_ap=0.155] 0.17048256215175933\n",
      "Epoch 6: 100%|██████████| 160/160 [01:12<00:00,  2.20it/s, v_num=229, train_loss=0.045, val_loss=0.0478, val_ap=0.170] 0.1800636016695451\n",
      "Epoch 7: 100%|██████████| 160/160 [01:12<00:00,  2.21it/s, v_num=229, train_loss=0.047, val_loss=0.0471, val_ap=0.180] 0.19498721599746166\n",
      "Epoch 8: 100%|██████████| 160/160 [01:12<00:00,  2.21it/s, v_num=229, train_loss=0.0476, val_loss=0.0464, val_ap=0.195]0.20002023884181547\n",
      "Epoch 9: 100%|██████████| 160/160 [01:12<00:00,  2.20it/s, v_num=229, train_loss=0.0435, val_loss=0.0461, val_ap=0.200]0.20702530349197323\n",
      "Epoch 10: 100%|██████████| 160/160 [01:12<00:00,  2.20it/s, v_num=229, train_loss=0.041, val_loss=0.0458, val_ap=0.207] 0.21805414420985145\n",
      "Epoch 11: 100%|██████████| 160/160 [01:12<00:00,  2.21it/s, v_num=229, train_loss=0.041, val_loss=0.045, val_ap=0.218] 0.22092163402224224\n",
      "Epoch 12: 100%|██████████| 160/160 [01:12<00:00,  2.20it/s, v_num=229, train_loss=0.0375, val_loss=0.045, val_ap=0.221]0.22142634591238763\n",
      "Epoch 13: 100%|██████████| 160/160 [01:13<00:00,  2.18it/s, v_num=229, train_loss=0.0407, val_loss=0.045, val_ap=0.221]0.22513377263454992\n",
      "Epoch 14: 100%|██████████| 160/160 [01:12<00:00,  2.21it/s, v_num=229, train_loss=0.0413, val_loss=0.045, val_ap=0.225]0.22626012735566928\n",
      "Epoch 15: 100%|██████████| 160/160 [01:12<00:00,  2.20it/s, v_num=229, train_loss=0.0389, val_loss=0.0449, val_ap=0.226]0.228062689169409\n",
      "Epoch 16: 100%|██████████| 160/160 [01:12<00:00,  2.20it/s, v_num=229, train_loss=0.0389, val_loss=0.0449, val_ap=0.228]0.23001118197393472\n",
      "Epoch 17: 100%|██████████| 160/160 [01:13<00:00,  2.18it/s, v_num=229, train_loss=0.0366, val_loss=0.0448, val_ap=0.230]0.22824536194000616\n",
      "Epoch 18: 100%|██████████| 160/160 [01:13<00:00,  2.19it/s, v_num=229, train_loss=0.0414, val_loss=0.0451, val_ap=0.228]0.23114243386215313\n",
      "Epoch 19: 100%|██████████| 160/160 [01:12<00:00,  2.19it/s, v_num=229, train_loss=0.037, val_loss=0.0451, val_ap=0.231] 0.22934802519766245\n",
      "Epoch 19: 100%|██████████| 160/160 [01:21<00:00,  1.95it/s, v_num=229, train_loss=0.037, val_loss=0.0454, val_ap=0.229]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 160/160 [01:22<00:00,  1.95it/s, v_num=229, train_loss=0.037, val_loss=0.0454, val_ap=0.229]\n"
     ]
    }
   ],
   "source": [
    "poolkernels = [(2, 1), (2, 1), (2, 1), (1, 1)]  # (1,1) = no pooling\n",
    "gru_in_out = [(768, 512), (512, 512), (512, 512)]\n",
    "layers = (len(gru_in_out))\n",
    "io_log = '-'.join([f\"{io[0]}x{io[1]}\" for io in gru_in_out])\n",
    "kernelslog = '-'.join([f\"{kernel[0]}x{kernel[1]}\" for kernel in poolkernels])\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=f'lightning_logs/grupool_l-{layers}_gruio-{io_log}_krnls-{kernelslog}/',\n",
    "                                      filename='{epoch}-{val_loss:.3f}-{val_ap:.3f}', \n",
    "                                      save_top_k=-1, \n",
    "                                      monitor=\"val_ap\", \n",
    "                                      every_n_epochs=1)\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=1, val_check_interval=1.0, \n",
    "                     max_epochs=20, log_every_n_steps=100,\n",
    "                     callbacks=[checkpoint_callback])\n",
    "model_6 = TrackToTags(grulayers=layers, gru_in_out=gru_in_out, poolkernels=poolkernels)\n",
    "trainer.fit(model_6, dataloader_train, dataloader_val)\n",
    "del model_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type              | Params\n",
      "-----------------------------------------------\n",
      "0 | grus     | ModuleList        | 936 K \n",
      "1 | maxpools | ModuleList        | 0     \n",
      "2 | bn       | LayerNorm         | 256   \n",
      "3 | relu     | ReLU              | 0     \n",
      "4 | fc       | Linear            | 33.0 K\n",
      "5 | loss     | BCEWithLogitsLoss | 0     \n",
      "-----------------------------------------------\n",
      "969 K     Trainable params\n",
      "0         Non-trainable params\n",
      "969 K     Total params\n",
      "3.878     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  5.41it/s]0.021899294496990688\n",
      "Epoch 0: 100%|██████████| 160/160 [01:10<00:00,  2.27it/s, v_num=230, train_loss=0.0878]0.025792186118671498\n",
      "Epoch 1: 100%|██████████| 160/160 [01:10<00:00,  2.27it/s, v_num=230, train_loss=0.0672, val_loss=0.0893, val_ap=0.0258]0.03108415726775836\n",
      "Epoch 2: 100%|██████████| 160/160 [01:10<00:00,  2.28it/s, v_num=230, train_loss=0.0644, val_loss=0.0686, val_ap=0.0311]0.039067939872245175\n",
      "Epoch 3: 100%|██████████| 160/160 [01:10<00:00,  2.28it/s, v_num=230, train_loss=0.0611, val_loss=0.0638, val_ap=0.0391]0.050199699634375476\n",
      "Epoch 4: 100%|██████████| 160/160 [01:10<00:00,  2.27it/s, v_num=230, train_loss=0.0563, val_loss=0.061, val_ap=0.0502] 0.06271868109419393\n",
      "Epoch 5: 100%|██████████| 160/160 [01:09<00:00,  2.31it/s, v_num=230, train_loss=0.0586, val_loss=0.0588, val_ap=0.0627]0.073731247377338\n",
      "Epoch 6: 100%|██████████| 160/160 [01:10<00:00,  2.27it/s, v_num=230, train_loss=0.054, val_loss=0.0571, val_ap=0.0737] 0.08572364305751604\n",
      "Epoch 7: 100%|██████████| 160/160 [01:10<00:00,  2.28it/s, v_num=230, train_loss=0.0545, val_loss=0.0556, val_ap=0.0857]0.09438721279876328\n",
      "Epoch 8: 100%|██████████| 160/160 [01:10<00:00,  2.27it/s, v_num=230, train_loss=0.0541, val_loss=0.0544, val_ap=0.0944]0.10536589327947365\n",
      "Epoch 9: 100%|██████████| 160/160 [01:10<00:00,  2.29it/s, v_num=230, train_loss=0.0519, val_loss=0.0532, val_ap=0.105] 0.11296167144052743\n",
      "Epoch 10: 100%|██████████| 160/160 [01:10<00:00,  2.28it/s, v_num=230, train_loss=0.0496, val_loss=0.0524, val_ap=0.113]0.11821183526903253\n",
      "Epoch 11: 100%|██████████| 160/160 [01:10<00:00,  2.28it/s, v_num=230, train_loss=0.0516, val_loss=0.0515, val_ap=0.118]0.12444783469653009\n",
      "Epoch 12: 100%|██████████| 160/160 [01:09<00:00,  2.29it/s, v_num=230, train_loss=0.0511, val_loss=0.051, val_ap=0.124] 0.1295179827630405\n",
      "Epoch 13: 100%|██████████| 160/160 [01:10<00:00,  2.28it/s, v_num=230, train_loss=0.0493, val_loss=0.0505, val_ap=0.130]0.1334816872862535\n",
      "Epoch 14: 100%|██████████| 160/160 [01:10<00:00,  2.28it/s, v_num=230, train_loss=0.0471, val_loss=0.0502, val_ap=0.133]0.13812103769759876\n",
      "Epoch 15: 100%|██████████| 160/160 [01:10<00:00,  2.28it/s, v_num=230, train_loss=0.0455, val_loss=0.0498, val_ap=0.138]0.1446416022049387\n",
      "Epoch 16: 100%|██████████| 160/160 [01:10<00:00,  2.28it/s, v_num=230, train_loss=0.0486, val_loss=0.0492, val_ap=0.145]0.1502519797214511\n",
      "Epoch 17: 100%|██████████| 160/160 [01:10<00:00,  2.27it/s, v_num=230, train_loss=0.0464, val_loss=0.0487, val_ap=0.150]0.1541034872021123\n",
      "Epoch 18: 100%|██████████| 160/160 [01:10<00:00,  2.26it/s, v_num=230, train_loss=0.0467, val_loss=0.0485, val_ap=0.154]0.15733488230849557\n",
      "Epoch 19: 100%|██████████| 160/160 [01:10<00:00,  2.28it/s, v_num=230, train_loss=0.0479, val_loss=0.0481, val_ap=0.157]0.16374220108272364\n",
      "Epoch 19: 100%|██████████| 160/160 [01:19<00:00,  2.02it/s, v_num=230, train_loss=0.0479, val_loss=0.0477, val_ap=0.164]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 160/160 [01:19<00:00,  2.02it/s, v_num=230, train_loss=0.0479, val_loss=0.0477, val_ap=0.164]\n"
     ]
    }
   ],
   "source": [
    "poolkernels = [(1, 1), (2, 1), (2, 1)]  # (1,1) = no pooling\n",
    "gru_in_out = [(768, 256), (256, 128)]\n",
    "layers = (len(gru_in_out))\n",
    "io_log = '-'.join([f\"{io[0]}x{io[1]}\" for io in gru_in_out])\n",
    "kernelslog = '-'.join([f\"{kernel[0]}x{kernel[1]}\" for kernel in poolkernels])\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=f'lightning_logs/grupool_l-{layers}_gruio-{io_log}_krnls-{kernelslog}/',\n",
    "                                      filename='{epoch}-{val_loss:.3f}-{val_ap:.3f}', \n",
    "                                      save_top_k=-1, \n",
    "                                      monitor=\"val_ap\", \n",
    "                                      every_n_epochs=1)\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=1, val_check_interval=1.0, \n",
    "                     max_epochs=20, log_every_n_steps=100,\n",
    "                     callbacks=[checkpoint_callback])\n",
    "model_7 = TrackToTags(grulayers=layers, gru_in_out=gru_in_out, poolkernels=poolkernels)\n",
    "trainer.fit(model_7, dataloader_train, dataloader_val)\n",
    "del model_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type              | Params\n",
      "-----------------------------------------------\n",
      "0 | grus     | ModuleList        | 3.5 M \n",
      "1 | maxpools | ModuleList        | 0     \n",
      "2 | bn       | LayerNorm         | 1.0 K \n",
      "3 | relu     | ReLU              | 0     \n",
      "4 | fc       | Linear            | 131 K \n",
      "5 | loss     | BCEWithLogitsLoss | 0     \n",
      "-----------------------------------------------\n",
      "3.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.7 M     Total params\n",
      "14.710    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  4.09it/s]0.027009973830844435\n",
      "Epoch 0: 100%|██████████| 200/200 [03:29<00:00,  0.95it/s, v_num=299, train_loss=0.0742]0.02284689214899853\n",
      "Epoch 1: 100%|██████████| 200/200 [02:51<00:00,  1.17it/s, v_num=299, train_loss=0.0721, val_loss=0.0723, val_ap=0.0228]0.028452053927475858\n",
      "Epoch 2: 100%|██████████| 200/200 [02:38<00:00,  1.26it/s, v_num=299, train_loss=0.0616, val_loss=0.0663, val_ap=0.0285]0.03687882860428941\n",
      "Epoch 3: 100%|██████████| 200/200 [02:35<00:00,  1.28it/s, v_num=299, train_loss=0.062, val_loss=0.0633, val_ap=0.0369] 0.05017514223552111\n",
      "Epoch 4: 100%|██████████| 200/200 [02:35<00:00,  1.29it/s, v_num=299, train_loss=0.0563, val_loss=0.0606, val_ap=0.0502]0.06442259701631109\n",
      "Epoch 5: 100%|██████████| 200/200 [02:35<00:00,  1.29it/s, v_num=299, train_loss=0.0608, val_loss=0.0584, val_ap=0.0644]0.07763534972659061\n",
      "Epoch 6: 100%|██████████| 200/200 [02:36<00:00,  1.27it/s, v_num=299, train_loss=0.0536, val_loss=0.0565, val_ap=0.0776]0.09046315129616674\n",
      "Epoch 7: 100%|██████████| 200/200 [02:34<00:00,  1.29it/s, v_num=299, train_loss=0.0528, val_loss=0.0549, val_ap=0.0905]0.10231763165376774\n",
      "Epoch 8: 100%|██████████| 200/200 [02:36<00:00,  1.27it/s, v_num=299, train_loss=0.0516, val_loss=0.0536, val_ap=0.102] 0.11358549936014582\n",
      "Epoch 9: 100%|██████████| 200/200 [02:36<00:00,  1.28it/s, v_num=299, train_loss=0.0495, val_loss=0.0523, val_ap=0.114]0.12421550745198906\n",
      "Epoch 10: 100%|██████████| 200/200 [02:43<00:00,  1.22it/s, v_num=299, train_loss=0.0498, val_loss=0.0514, val_ap=0.124]0.13225011087571675\n",
      "Epoch 11: 100%|██████████| 200/200 [02:41<00:00,  1.24it/s, v_num=299, train_loss=0.0503, val_loss=0.0507, val_ap=0.132]0.13950668927873044\n",
      "Epoch 12: 100%|██████████| 200/200 [02:40<00:00,  1.24it/s, v_num=299, train_loss=0.0474, val_loss=0.0501, val_ap=0.140]0.14526772451519604\n",
      "Epoch 13: 100%|██████████| 200/200 [02:41<00:00,  1.24it/s, v_num=299, train_loss=0.0493, val_loss=0.0496, val_ap=0.145]0.1538691213430818\n",
      "Epoch 14: 100%|██████████| 200/200 [02:47<00:00,  1.19it/s, v_num=299, train_loss=0.0497, val_loss=0.049, val_ap=0.154] 0.1602892941707671\n",
      "Epoch 15: 100%|██████████| 200/200 [02:42<00:00,  1.23it/s, v_num=299, train_loss=0.0475, val_loss=0.0484, val_ap=0.160]0.1665876350771101\n",
      "Epoch 16: 100%|██████████| 200/200 [02:47<00:00,  1.20it/s, v_num=299, train_loss=0.0468, val_loss=0.048, val_ap=0.167] 0.17381369074454564\n",
      "Epoch 17: 100%|██████████| 200/200 [02:49<00:00,  1.18it/s, v_num=299, train_loss=0.0467, val_loss=0.0476, val_ap=0.174]0.18021024738826436\n",
      "Epoch 18: 100%|██████████| 200/200 [02:40<00:00,  1.25it/s, v_num=299, train_loss=0.0461, val_loss=0.0471, val_ap=0.180]0.18516143323771453\n",
      "Epoch 19: 100%|██████████| 200/200 [02:49<00:00,  1.18it/s, v_num=299, train_loss=0.0436, val_loss=0.0467, val_ap=0.185]0.19278726929829407\n",
      "Epoch 20: 100%|██████████| 200/200 [02:45<00:00,  1.21it/s, v_num=299, train_loss=0.0449, val_loss=0.0461, val_ap=0.193]0.1985159578357052\n",
      "Epoch 21: 100%|██████████| 200/200 [02:50<00:00,  1.17it/s, v_num=299, train_loss=0.0462, val_loss=0.0456, val_ap=0.199]0.2018946529858195\n",
      "Epoch 22: 100%|██████████| 200/200 [03:13<00:00,  1.03it/s, v_num=299, train_loss=0.0448, val_loss=0.0455, val_ap=0.202]0.20641893496746389\n",
      "Epoch 23: 100%|██████████| 200/200 [03:07<00:00,  1.07it/s, v_num=299, train_loss=0.0433, val_loss=0.0452, val_ap=0.206]0.20881014242588108\n",
      "Epoch 24: 100%|██████████| 200/200 [03:21<00:00,  0.99it/s, v_num=299, train_loss=0.0463, val_loss=0.045, val_ap=0.209] 0.21222040962190164\n",
      "Epoch 25: 100%|██████████| 200/200 [03:13<00:00,  1.03it/s, v_num=299, train_loss=0.0467, val_loss=0.0448, val_ap=0.212]0.21544195018275486\n",
      "Epoch 26: 100%|██████████| 200/200 [03:04<00:00,  1.08it/s, v_num=299, train_loss=0.0459, val_loss=0.0446, val_ap=0.215]0.21877060466517526\n",
      "Epoch 27: 100%|██████████| 200/200 [03:10<00:00,  1.05it/s, v_num=299, train_loss=0.0457, val_loss=0.0444, val_ap=0.219]0.22152951187743702\n",
      "Epoch 28: 100%|██████████| 200/200 [03:10<00:00,  1.05it/s, v_num=299, train_loss=0.0427, val_loss=0.0442, val_ap=0.222]0.22509891082852446\n",
      "Epoch 29: 100%|██████████| 200/200 [03:08<00:00,  1.06it/s, v_num=299, train_loss=0.0424, val_loss=0.044, val_ap=0.225] 0.2273137351950222\n",
      "Epoch 29: 100%|██████████| 200/200 [03:20<00:00,  1.00it/s, v_num=299, train_loss=0.0424, val_loss=0.0439, val_ap=0.227]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 200/200 [03:20<00:00,  1.00it/s, v_num=299, train_loss=0.0424, val_loss=0.0439, val_ap=0.227]\n"
     ]
    }
   ],
   "source": [
    "poolkernels = [(1, 1), (3, 1), (3, 1)]  # (1,1) = no pooling\n",
    "gru_in_out = [(768, 512), (512, 512)]\n",
    "layers = (len(gru_in_out))\n",
    "io_log = '-'.join([f\"{io[0]}x{io[1]}\" for io in gru_in_out])\n",
    "kernelslog = '-'.join([f\"{kernel[0]}x{kernel[1]}\" for kernel in poolkernels])\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=f'lightning_logs/trv_grupoolaccum_l-{layers}_gruio-{io_log}_krnls-{kernelslog}/',\n",
    "                                      filename='{epoch}-{val_loss:.3f}-{val_ap:.3f}', \n",
    "                                      save_top_k=-1, \n",
    "                                      monitor=\"val_ap\", \n",
    "                                      every_n_epochs=1)\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=1, val_check_interval=1.0, \n",
    "                     max_epochs=30, log_every_n_steps=100,\n",
    "                     accumulate_grad_batches=1024//batch_size,\n",
    "                     callbacks=[checkpoint_callback]\n",
    "                     )\n",
    "model_7 = TrackToTags(grulayers=layers, gru_in_out=gru_in_out, poolkernels=poolkernels)\n",
    "trainer.fit(model_7, dataloader_trainval, dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poolkernels = [(1, 1), (3, 1), (3, 1)]  # (1,1) = no pooling\n",
    "gru_in_out = [(768, 512), (512, 512)]\n",
    "layers = (len(gru_in_out))\n",
    "io_log = '-'.join([f\"{io[0]}x{io[1]}\" for io in gru_in_out])\n",
    "kernelslog = '-'.join([f\"{kernel[0]}x{kernel[1]}\" for kernel in poolkernels])\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=f'lightning_logs/grupoolworelu_l-{layers}_gruio-{io_log}_krnls-{kernelslog}/',\n",
    "                                      filename='{epoch}-{val_loss:.3f}-{val_ap:.3f}', \n",
    "                                      save_top_k=-1, \n",
    "                                      monitor=\"val_ap\", \n",
    "                                      every_n_epochs=1)\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=1, val_check_interval=1.0, \n",
    "                     max_epochs=23, log_every_n_steps=100,\n",
    "                     callbacks=[checkpoint_callback]\n",
    "                     )\n",
    "model_8 = TrackToTags(grulayers=layers, gru_in_out=gru_in_out, poolkernels=poolkernels, relu_on=False)\n",
    "trainer.fit(model_8, dataloader_train, dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2eddd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, loader):\n",
    "    model.to(global_device)\n",
    "    model.eval()\n",
    "    track_idxs = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            track_idx, embeds = data\n",
    "            pred_logits = model(embeds)\n",
    "            pred_probs = torch.sigmoid(pred_logits)\n",
    "            predictions.append(pred_probs.cpu().numpy())\n",
    "            track_idxs.append(track_idx.numpy())\n",
    "    predictions = np.vstack(predictions)\n",
    "    track_idxs = np.vstack(track_idxs).ravel()\n",
    "    return track_idxs, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for metalearning\n",
    "poolkernels = [(1, 1), (1, 1)]\n",
    "gru_in_out = [(768, 512)]\n",
    "layers = (len(gru_in_out))\n",
    "model_1 = TrackToTags.load_from_checkpoint('lightning_logs/grupool_l-1_gruio-768x512_krnls-1x1-1x1/epoch=9-val_loss=0.045-val_ap=0.236.ckpt', \n",
    "                                           grulayers=layers, gru_in_out=gru_in_out, poolkernels=poolkernels)\n",
    "\n",
    "poolkernels = [(1, 1), (3, 1), (3, 1)]\n",
    "gru_in_out = [(768, 512), (512, 512)]\n",
    "layers = (len(gru_in_out))\n",
    "model_3 = TrackToTags.load_from_checkpoint('lightning_logs/grupool_l-2_gruio-768x512-512x512_krnls-1x1-3x1-3x1/epoch=19-val_loss=0.045-val_ap=0.238.ckpt', \n",
    "                                           grulayers=layers, gru_in_out=gru_in_out, poolkernels=poolkernels)\n",
    "\n",
    "model_7 = TrackToTags.load_from_checkpoint('lightning_logs/grupoolaccum2_l-2_gruio-768x512-512x512_krnls-1x1-3x1-3x1/epoch=23-val_loss=0.044-val_ap=0.241.ckpt', \n",
    "                                           grulayers=layers, gru_in_out=gru_in_out, poolkernels=poolkernels)\n",
    "\n",
    "# model_8 = TrackToTags.load_from_checkpoint('lightning_logs/grupoolworelu_l-2_gruio-768x512-512x512_krnls-1x1-3x1-3x1/epoch=25-val_loss=0.045-val_ap=0.239.ckpt', \n",
    "#                                            grulayers=layers, gru_in_out=gru_in_out, poolkernels=poolkernels, relu_on=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "poolkernels = [(1, 1), (1, 1)]\n",
    "gru_in_out = [(768, 512)]\n",
    "layers = (len(gru_in_out))\n",
    "model_1 = TrackToTags.load_from_checkpoint('lightning_logs/trv_grupool_l-1_gruio-768x512_krnls-1x1-1x1/epoch=29-val_loss=0.037-val_ap=0.356.ckpt', \n",
    "                                           grulayers=layers, gru_in_out=gru_in_out, poolkernels=poolkernels)\n",
    "\n",
    "\n",
    "\n",
    "poolkernels = [(1, 1), (3, 1), (3, 1)]\n",
    "gru_in_out = [(768, 512), (512, 512)]\n",
    "layers = (len(gru_in_out))\n",
    "# 22 > 20\n",
    "model_3 = TrackToTags.load_from_checkpoint('lightning_logs/trv_grupool_l-2_gruio-768x512-512x512_krnls-1x1-3x1-3x1/epoch=22-val_loss=0.034-val_ap=0.414.ckpt', \n",
    "                                           grulayers=layers, gru_in_out=gru_in_out, poolkernels=poolkernels)\n",
    "\n",
    "model_7 = TrackToTags.load_from_checkpoint('lightning_logs/trv_grupoolaccum_l-2_gruio-768x512-512x512_krnls-1x1-3x1-3x1/epoch=20-val_loss=0.046-val_ap=0.199.ckpt', \n",
    "                                           grulayers=layers, gru_in_out=gru_in_out, poolkernels=poolkernels)\n",
    "\n",
    "# model_8 = TrackToTags.load_from_checkpoint('lightning_logs/grupoolworelu_l-2_gruio-768x512-512x512_krnls-1x1-3x1-3x1/epoch=25-val_loss=0.045-val_ap=0.239.ckpt', \n",
    "#                                            grulayers=layers, gru_in_out=gru_in_out, poolkernels=poolkernels, relu_on=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10227, 256), (10227, 1024))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_list = [model_1, model_3, model_7, model_8]\n",
    "meta_factors_list = []   # num_models x [val_size x 256]\n",
    "for model in models_list:\n",
    "    track_idxs, preds = predict(model, dataloader_val_astest)\n",
    "    meta_factors_list.append(preds)\n",
    "val_true = np.array([y[2] for y in dataset_val])  # val_size x 256\n",
    "meta_factors = np.concatenate(meta_factors_list, axis=1)  # val_size x 256*num_models\n",
    "val_true.shape, meta_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, ground_true=None):\n",
    "        self.ground_true = ground_true\n",
    "        self.X = X\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.ground_true is None:\n",
    "            return self.X[idx]\n",
    "        return self.X[idx], self.ground_true[idx]\n",
    "    \n",
    "\n",
    "def metacollate(batch):\n",
    "    meta_factors = torch.FloatTensor(np.vstack([b[0] for b in batch])).to(global_device)\n",
    "    ground_trues = torch.FloatTensor(np.vstack([b[1] for b in batch])).to(global_device)\n",
    "    return meta_factors, ground_trues\n",
    "\n",
    "def metacollate_test(batch):\n",
    "    meta_factors = torch.FloatTensor(np.vstack(batch)).to(global_device)\n",
    "    return meta_factors\n",
    "\n",
    "\n",
    "metadataset_train = MetaDataset(meta_factors, ground_true=val_true)\n",
    "\n",
    "metadataloader_train = DataLoader(metadataset_train, batch_size, shuffle=True, collate_fn=metacollate)\n",
    "metadataloader_val = DataLoader(metadataset_train, batch_size, shuffle=False, collate_fn=metacollate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaModel(pl.LightningModule):\n",
    "    def __init__(self, num_models, num_classes=256, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        input_dim = num_classes * num_models\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "        )\n",
    "        # self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        self.metric = average_precision_score\n",
    "        self._transit_val = {'preds': [], 'labels': []}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outs = self.lin(x)\n",
    "        # outs = self.fc(x)\n",
    "        return outs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        meta_factors, ground_trues = batch\n",
    "        pred_logits = self(meta_factors)\n",
    "        loss = self.loss(pred_logits, ground_trues)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        meta_factors, ground_trues = batch\n",
    "        pred_logits = self(meta_factors)\n",
    "        loss = self.loss(pred_logits, ground_trues)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        \n",
    "        pred_probs = torch.sigmoid(pred_logits)\n",
    "        self._transit_val['labels'].append(np.array(ground_trues.int().cpu()))\n",
    "        self._transit_val['preds'].append(np.array(pred_probs.cpu()))\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        preds = np.vstack(self._transit_val['preds'])\n",
    "        labels = np.vstack(self._transit_val['labels'])\n",
    "        ap = self.metric(labels, preds)\n",
    "        self.log('val_ap', ap, prog_bar=True)\n",
    "        self._transit_val['labels'] = []\n",
    "        self._transit_val['preds'] = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=3e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)\n",
    "        \n",
    "        return {\"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                                \"scheduler\": scheduler,\n",
    "                                \"interval\": \"epoch\",\n",
    "                                \"frequency\": 10\n",
    "                                },\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type              | Params\n",
      "-------------------------------------------\n",
      "0 | lin  | Sequential        | 657 K \n",
      "1 | loss | BCEWithLogitsLoss | 0     \n",
      "-------------------------------------------\n",
      "657 K     Trainable params\n",
      "0         Non-trainable params\n",
      "657 K     Total params\n",
      "2.629     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 40/40 [00:01<00:00, 24.63it/s, v_num=295, train_loss=0.041, val_loss=0.0403, val_ap=0.308]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=70` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 40/40 [00:01<00:00, 23.81it/s, v_num=295, train_loss=0.041, val_loss=0.0403, val_ap=0.308]\n"
     ]
    }
   ],
   "source": [
    "num_models = len(models_list)\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=f'lightning_logs/metamodel_{num_models}models/',\n",
    "                                      filename='{epoch}-{val_loss:.3f}-{val_ap:.3f}', \n",
    "                                      save_top_k=-1, \n",
    "                                      monitor=\"val_ap\", \n",
    "                                      every_n_epochs=1)\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=1, val_check_interval=1.0, \n",
    "                     max_epochs=70, log_every_n_steps=100,\n",
    "                     callbacks=[checkpoint_callback])\n",
    "metamodel = MetaModel(num_models=num_models)\n",
    "trainer.fit(metamodel, metadataloader_train, metadataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "metamodel = MetaModel.load_from_checkpoint(f'lightning_logs/metamodel_{num_models}models/epoch=38-val_loss=0.041-val_ap=0.299.ckpt', \n",
    "                                            num_models=num_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metapredict(model, loader):\n",
    "    model.to(global_device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            pred_logits = model(data)\n",
    "            pred_probs = torch.sigmoid(pred_logits)\n",
    "            predictions.append(pred_probs.cpu().numpy())\n",
    "    predictions = np.vstack(predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "067b1acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_factors_list = []   # num_models x [test_size x 256]\n",
    "for model in models_list:\n",
    "    track_idxs, preds = predict(model.to(global_device), dataloader_test)\n",
    "    meta_factors_list.append(preds)\n",
    "meta_factors = np.concatenate(meta_factors_list, axis=1)  # test_size x 256*num_models\n",
    "metadataset_test = MetaDataset(meta_factors)\n",
    "metadataloader_test = DataLoader(metadataset_test, batch_size, shuffle=False, collate_fn=metacollate_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = metapredict(metamodel, metadataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_idxs, predictions = predict(model_3.to(global_device), dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "718719fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame([\n",
    "    {'track': track, 'prediction': ','.join([str(p) for p in probs])}\n",
    "    for track, probs in zip(track_idxs, predictions)\n",
    "])\n",
    "predictions_df.to_csv('results/prediction_mod3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, preds in enumerate(meta_factors_list):\n",
    "    predictions_df = pd.DataFrame([\n",
    "    {'track': track, 'prediction': ','.join([str(p) for p in probs])}\n",
    "    for track, probs in zip(track_idxs, preds)\n",
    "    ])\n",
    "    predictions_df.to_csv(f'results/prediction_{i}.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
